task:
  id: "r2-t4-result-aggregation"
  name: "Build Result Aggregation Service"
  session_size: "Complete aggregation system with weighted scoring and business rules"
  
  context:
    why: "Combines specialist agent outputs into final compliance scores and recommendations"
    integrates_with: ["orchestration-engine", "agent-factory", "database-schema"]
    references:
      - "masterplan aggregation logic (lines 2902-3080)"  # CORRECTED: Updated line numbers
      - "orchestrator aggregate_results method (lines 606-620)"  # ADDED: Specific method reference
      - "TahoeAgent output format (lines 836-845)"  # ADDED: Agent output format reference
    
  build_spec:
    creates:
      - "services/agent-engine/src/services/aggregation.py"
      # CORRECTED: Removed results.py - AnalysisResult is in aggregation.py
      - "services/agent-engine/tests/test_aggregation.py"
      - "services/agent-engine/tests/fixtures/agent_results.json"
    modifies:
      - "src/orchestrator.py (use real aggregator - replace stub)"
      # CORRECTED: AnalysisResult already exists, no API model changes needed
      - "scripts/seed.py (add aggregation rules to scorecard data)"
    uses:
      - "Weighted scoring algorithms"
      - "Business rule evaluation"
      - "Threshold-based categorization"
    
  implementation_guide:
    start_with: "Create ResultAggregator class matching masterplan interface (lines 2925-2930)"
    core_logic:
      # CORRECTED: Process dict results from TahoeAgent, not AgentResult objects
      - "Process dict agent results from orchestrator execution"
      - "Weighted score calculation using scorecard agent weights"
      - "Confidence score combination from individual agents"
      - "Violation deduplication and severity ranking"
      - "Recommendation prioritization and deduplication"
      - "Category scoring (agent-specific results)"
      - "Pass/fail/review determination using scorecard thresholds"
      - "Audit trail generation with execution metadata"
    connects_to:
      # CORRECTED: Align with actual orchestrator interface
      - "Dict agent results from execute_agents (lines 604, 588, 600)"
      - "Scorecard aggregationRules and thresholds (lines 120, 119)"
      - "Database for scorecard configuration loading"
    keeps_simple:
      - "Basic weighted average for scores"
      - "Simple confidence calculation using statistics.mean"
      - "No ML-based aggregation"
      - "Fixed business rules from thresholds"
      - "Basic violation merging by type and severity"
    
  local_validation:
    run_commands:
      - "python -m pytest tests/test_aggregation.py -v"
      - "python scripts/test_aggregation.py"
    verify_endpoints:
      # CORRECTED: Match exact orchestrator interface
      - "Aggregator.aggregate(agent_results, aggregation_rules, thresholds)"
      - "Scores calculated with agent weights from scorecard"
      - "Thresholds applied for pass/fail determination"
    check_functionality:
      - "Weighted scores match scorecard configuration"
      - "Violations deduplicated by type and severity"
      - "Recommendations prioritized by impact"
      - "Categories contain per-agent results"
      - "Audit trail includes execution metadata"
      # ADDED: Integration validation
      - "AnalysisResult compatible with orchestrator return type"
      - "Business rules applied correctly per scorecard thresholds"
    
  session_notes:
    context_critical:
      # CORRECTED: Accurate data flow description
      - "Agent results are dicts with agent_name, score, confidence, violations, recommendations"
      - "Weights from scorecard.scorecardAgents[].weight (lines 138)"
      - "Thresholds from scorecard.thresholds JSON field (line 119)"
      - "Confidence affects final score calculation"
      - "Audit trail required for compliance tracking"
    remember_for_later:
      - "Aggregation rules are configurable per scorecard"
      - "Different scorecards = different aggregation logic"
      - "Categories contain agent-specific sub-scores"
      - "Violations need evidence and location data"

  implementation_checklist:
    - "Create ResultAggregator class with exact masterplan interface"
    # CORRECTED: Process dict format from TahoeAgent
    - "Implement weighted scoring for TahoeAgent dict results"
    - "Build confidence calculation from agent dict outputs"
    - "Add violation processing from TahoeAgent findings"
    - "Create recommendation logic from TahoeAgent suggestions"
    - "Implement category scoring per agent"
    - "Add threshold evaluation from scorecard configuration"
    - "Generate comprehensive audit trails"
    - "Write tests with realistic TahoeAgent dict outputs"
    # CORRECTED: Integration with actual systems
    - "Validate with orchestrator's aggregate_results method signature"
    - "Test business rules application with scorecard thresholds"

  # CORRECTED: Use actual TahoeAgent dict format
  agent_result_format:
    tahoe_agent_dict: |
      {
        "agent_name": "compliance_specialist",
        "agent_version": "1.0",
        "result": "LLM analysis response text",
        "events": [...],  # ADK events
        "confidence": 0.85,
        "execution_time": 2.3,
        "model_used": "gemini-2.0-flash",
        "trace_id": "uuid",
        # ADDED: Fields extracted from LLM response
        "score": 75.0,
        "violations": [...],
        "recommendations": [...],
        "findings": [...]
      }

  aggregation_logic:
    weighted_score: |
      # CORRECTED: Use scorecard agent weights
      total_weight = sum(
          scorecard_agent["weight"] 
          for scorecard_agent in scorecard["scorecardAgents"]
      )
      weighted_sum = sum(
          result["score"] * scorecard_agent["weight"] 
          for agent_name, result in agent_results.items()
          for scorecard_agent in scorecard["scorecardAgents"]
          if scorecard_agent["agent"]["name"] == agent_name
      )
      overall_score = weighted_sum / total_weight
    
    confidence_calculation: |
      # CORRECTED: From TahoeAgent dict format
      confidences = [
          result["confidence"] 
          for result in agent_results.values() 
          if "error" not in result
      ]
      overall_confidence = statistics.mean(confidences) if confidences else 0.5
    
    violation_processing: |
      # Deduplicate by violation type and severity
      all_violations = []
      for result in agent_results.values():
          all_violations.extend(result.get("violations", []))
      
      # Apply business rules for critical violations
      critical_violations = [v for v in all_violations if v.get("severity") == "critical"]
      if critical_violations:
          # Cap score for critical violations
          final_score = min(final_score, thresholds.get("max_score_with_critical", 50))
    
    recommendation_priority: |
      # CORRECTED: From TahoeAgent dict format
      all_recommendations = []
      for result in agent_results.values():
          all_recommendations.extend(result.get("recommendations", []))
      
      # Deduplicate and sort by priority
      priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
      sorted_recs = sorted(
          all_recommendations,
          key=lambda x: priority_order.get(x.get("priority", "low"), 3)
      )

  business_rules:
    pass_fail_logic: |
      # CORRECTED: Use scorecard thresholds structure
      thresholds = scorecard["thresholds"]
      
      if overall_score >= thresholds["pass"]:
          status = "PASS"
      elif overall_score >= thresholds["review"]:
          status = "REVIEW"  
      else:
          status = "FAIL"
    
    category_scoring: |
      # CORRECTED: Per-agent categorization
      categories = {}
      for agent_name, result in agent_results.items():
          if "error" not in result:
              categories[agent_name] = {
                  "score": result["score"],
                  "confidence": result["confidence"],
                  "findings": result.get("findings", []),
                  "agent_version": result["agent_version"]
              }

  result_structure:
    AnalysisResult:
      # CORRECTED: Match masterplan AnalysisResult dataclass (lines 2911-2920)
      - "analysis_id: str"
      - "overall_score: float (0-100)"
      - "confidence: float (0-1)"
      - "categories: Dict[str, Any] (per-agent results)"
      - "violations: List[Dict[str, Any]]"
      - "recommendations: List[Dict[str, Any]]"
      - "audit_trail: Dict[str, Any]"
      - "execution_time: float"
      
  # ADDED: Missing database integration requirements
  database_integration:
    scorecard_loading:
      - "Load aggregationRules from scorecard.aggregationRules JSON field"
      - "Load thresholds from scorecard.thresholds JSON field"
      - "Access agent weights from scorecard.scorecardAgents[].weight"
    
    audit_requirements:
      - "Track agents_executed, agents_successful, agents_failed"
      - "Record aggregation_method and thresholds_applied"
      - "Include execution timing and trace information"