task:
  id: "r2-t1-orchestration-engine"
  name: "Build Core Agent Orchestration Engine"
  session_size: "Complete orchestration workflow with agent selection and execution planning"
  
  context:
    why: "Central coordination of multi-agent compliance analysis workflow"
    integrates_with: ["agent-factory", "result-aggregation", "scorecard-management", "redis-cache"]
    references:
      - "masterplan orchestration engine (lines 305-686)"
      - "roadmap phase 2 orchestration requirements"
    
  build_spec:
    creates:
      # CORRECTED: Removed services/agent-engine/src/services/analysis.py - not in masterplan
      - "services/agent-engine/src/services/content_analyzer.py"
      - "services/agent-engine/src/agents/factory.py"  # ADDED: Stub for AgentFactory
      - "services/agent-engine/src/services/aggregation.py"  # ADDED: Stub for ResultAggregator
      - "services/agent-engine/tests/test_orchestrator.py"
      - "services/agent-engine/tests/fixtures/fdcpa_violation.json"
    modifies:
      - "src/orchestrator.py (complete implementation)"
      - "src/models/api.py (add AnalysisResult model)"
      - "requirements.txt (ensure google-adk included)"  # CORRECTED: ADK needed from start
    uses:
      - "Google ADK (LlmAgent, Runner, FunctionTool)"  # VERIFIED: Core ADK components per docs
      - "Asyncio for parallel execution"
      - "Redis for session management"
      - "Prisma for state persistence"
    
  implementation_guide:
    start_with: "Complete TahoeOrchestrator class with full workflow"
    core_logic:
      - "analyze_interaction main workflow method"
      - "Content analysis to determine requirements"
      - "Agent selection based on scorecard rules"
      - "Execution plan building (parallel/sequential)"
      - "Session management in Redis"
      - "Error handling and recovery"
      - "Phase tracking and status updates"
    imports_required:  # VERIFIED: ADK docs confirm these imports
      - "from google.adk.agents import LlmAgent"  # CORRECTED: Use LlmAgent not Agent per ADK docs
      - "from google.adk.agents.workflow import ParallelAgent, SequentialAgent"  # VERIFIED: Workflow agents
      - "from google.adk.tools import FunctionTool"  # CORRECTED: Use FunctionTool not @tool decorator
      - "from google.adk.runner import Runner"  # ADDED: Required for agent execution per ADK docs
      - "from google.adk.sessions import InMemorySessionService"  # ADDED: For session management
      - "import redis.asyncio as redis"
      - "from prisma import Prisma"
    helper_methods:  # ADDED: Required helper methods from masterplan
      - "_should_activate_agent (lines 638-662)"
      - "_group_by_execution_order (lines 664-674)"
      - "_update_session_phase (lines 676-685)"
      - "_execute_single_agent (lines 609-636)"
    connects_to:
      - "Agent factory for creating agents (stub class)"  # CORRECTED: Create stub
      - "Result aggregator for combining outputs (stub class)"  # CORRECTED: Create stub
      - "Redis for real-time session tracking"
      - "Database for configuration and results"
    keeps_simple:
      - "Stub AgentFactory with mock create_agent method"  # CORRECTED: Use stubs not pure mocks
      - "Stub ResultAggregator with mock aggregate method"  # CORRECTED: Use stubs
      - "Basic content analysis (keywords only)"
      - "Simple parallel execution (no optimization)"
      - "Fixed timeout values (30 seconds)"
      - "Basic error handling (log and fail)"
    
  local_validation:
    run_commands:
      - "python -m pytest tests/test_orchestrator.py -v"
      - "python scripts/test_orchestration.py"
      - "redis-cli get 'analysis:session:*'"
    verify_endpoints:
      - "POST /analyze creates session in Redis"
      - "Analysis phases tracked in session"
      - "Database record updated with results"
    check_functionality:
      - "Can orchestrate simple workflow end-to-end"
      - "Sessions created and cleaned up in Redis"
      - "Agent selection based on scorecard"
      - "Execution plan groups agents correctly"
      - "Errors handled gracefully"
    
  session_notes:
    context_critical:
      - "Session state enables monitoring"
      - "Phases: content_analysis → agent_selection → execution → aggregation"
      - "Each agent gets 30-second timeout"
      - "Cache scorecard configurations"
      - "Include all helper methods from masterplan"  # ADDED
    remember_for_later:
      - "Agent factory interface for R2-T2"
      - "Result aggregator interface for R2-T4"
      - "Session format for monitoring"
      - "Error handling patterns"

  implementation_checklist:
    - "Import ADK classes (LlmAgent, Runner, FunctionTool)"  # VERIFIED: Core ADK imports
    - "Complete analyze_interaction workflow"
    - "Implement analyze_content method"
    - "Build select_agents with trigger rules"
    - "Create build_execution_plan"
    - "Implement execute_agents (with stub factory)"  # CORRECTED
    - "Add _should_activate_agent helper"  # ADDED
    - "Add _group_by_execution_order helper"  # ADDED
    - "Add _update_session_phase helper"  # ADDED
    - "Add _execute_single_agent helper"  # ADDED
    - "Add Redis session management"
    - "Create ContentAnalyzer service"
    - "Create AgentFactory stub (with TahoeAgent wrapper)"  # VERIFIED: Matches masterplan pattern
    - "Create ResultAggregator stub (with AnalysisResult)"  # VERIFIED: Returns proper result class
    - "Write comprehensive tests"
    - "Add fixture for FDCPA violation"

  workflow_phases:
    1_initialization:
      - "Generate trace_id"
      - "Create analysis record"
      - "Initialize Redis session"
    
    2_content_analysis:
      - "Update session phase via helper"  # CORRECTED: Use helper method
      - "Detect language"
      - "Extract topics (keywords)"
      - "Identify regulatory indicators"
      - "Calculate complexity score"
    
    3_agent_selection:
      - "Load scorecard configuration"
      - "Check agent trigger rules via _should_activate_agent"  # CORRECTED: Use helper
      - "Build required agents list"
      - "Sort by execution order"
    
    4_execution_planning:
      - "Separate parallel vs sequential"
      - "Group by execution order via _group_by_execution_order"  # CORRECTED: Use helper
      - "Calculate total agents"
    
    5_agent_execution:
      - "Update session phase"  # ADDED
      - "Execute parallel agents first"
      - "Then sequential phases"
      - "Use _execute_single_agent for each"  # CORRECTED: Use helper
      - "Handle timeouts and errors"
      - "Pass context between agents"
    
    6_result_aggregation:
      - "Update session phase"  # ADDED
      - "Call ResultAggregator stub"  # CORRECTED
      - "Return aggregated results"
    
    7_cleanup:
      - "Update database record"
      - "Clear Redis session"
      - "Return results"

  stub_implementations:  # VERIFIED: ADK-compliant stub implementations
    agent_factory_stub: |
      # src/agents/factory.py - ADK-compliant stub implementation
      from google.adk.agents import LlmAgent  # VERIFIED: Correct ADK import
      from google.adk.tools import FunctionTool  # VERIFIED: For tool wrapping
      from prisma import Prisma
      import redis.asyncio as redis
      
      class AgentFactory:
          """Factory for creating ADK agents from database templates"""
          
          def __init__(self):
              self.db = Prisma()
              self.cache = redis.Redis()
              self.model_registry = None  # Will be ModelRegistry()
              self.tool_registry = None  # Will be ToolRegistry()
              
          async def create_agent(self, template: dict):
              """Create a stub agent for testing orchestration"""
              # Return a TahoeAgent wrapper with mock behavior
              class TahoeAgent:
                  def __init__(self):
                      self.template = template
                      
                  async def analyze(self, input_data: dict):
                      # Mock analysis result matching ADK patterns
                      return {
                          "agent_name": template["name"],
                          "agent_version": template.get("version", 1),
                          "result": {"mock": "result"},
                          "events": [],  # Would contain ADK events
                          "confidence": 0.9,
                          "execution_time": 1.5,
                          "model_used": template.get("model", "gemini-2.0-flash"),
                          "trace_id": input_data.get("trace_id")
                      }
              
              return TahoeAgent()
    
    result_aggregator_stub: |
      # src/services/aggregation.py - Stub implementation  
      class ResultAggregator:
          async def aggregate(self, agent_results, aggregation_rules, thresholds):
              # Simple aggregation for testing
              scores = [r.get("score", 0) for r in agent_results.values() if isinstance(r, dict)]
              overall_score = sum(scores) / len(scores) if scores else 0
              
              class AnalysisResult:
                  def __init__(self):
                      self.analysis_id = "test-analysis-id"
                      self.overall_score = overall_score
                      self.confidence = 0.9
                      self.violations = []
                      self.recommendations = []
                      self.categories = {}
                      self.audit_trail = {}
                      self.execution_time = 2.5
                      
                  def to_dict(self):
                      return {
                          "overall_score": self.overall_score,
                          "confidence": self.confidence
                      }
              
              return AnalysisResult()
    
    content_analyzer_stub: |
      # src/services/content_analyzer.py - Basic implementation
      class ContentAnalyzer:
          def detect_language(self, interaction_data):
              return "en"
              
          async def extract_topics(self, interaction_data):
              content = interaction_data.get("content", "").lower()
              topics = []
              if "payment" in content:
                  topics.append("payment")
              if "collection" in content:
                  topics.append("collection")
              return topics
              
          async def detect_regulatory_context(self, interaction_data):
              content = interaction_data.get("content", "").lower()
              indicators = []
              if "debt" in content or "collection" in content:
                  indicators.append("fdcpa")
              return indicators
              
          async def assess_complexity(self, interaction_data):
              content_length = len(interaction_data.get("content", ""))
              if content_length < 500:
                  return 0.3
              elif content_length < 1500:
                  return 0.6
              else:
                  return 0.9