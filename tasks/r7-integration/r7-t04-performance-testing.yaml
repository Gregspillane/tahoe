task:
  id: "r7-t04-performance-testing"
  name: "Implement Performance Testing and Optimization"
  description: "Create comprehensive performance testing suite with load testing, benchmarks, and optimization recommendations"
  complexity: "complex"
  estimated_hours: 4
  
  context:
    why: "Performance testing ensures system scalability, identifies bottlenecks, and validates production readiness"
    architectural_role: "Quality assurance layer for performance validation and optimization"
    depends_on_tasks: ["r7-t01", "r7-t02", "r6-t04"]
    enables_tasks: []
    references:
      masterplan: "@MASTERPLAN.md#performance-requirements"
      architecture: "@memory-bank/architecture.md#performance-optimization"
    
  implementation:
    creates:
      - path: "performance/"
        purpose: "Performance testing and optimization"
        content:
          - "tests/"
          - "benchmarks/"
          - "reports/"
          - "scripts/"
          - "config/"
      
      - path: "performance/tests/"
        purpose: "Performance test suites"
        content:
          - "load_tests.py"
          - "stress_tests.py"
          - "spike_tests.py"
          - "endurance_tests.py"
          - "api_benchmarks.py"
          - "websocket_tests.py"
      
      - path: "performance/tests/load_tests.py"
        purpose: "Load testing scenarios"
        content_structure: |
          """Load testing scenarios for Tahoe API"""
          
          import asyncio
          import time
          import statistics
          from typing import Dict, List, Any, Optional
          import aiohttp
          import json
          from dataclasses import dataclass
          from datetime import datetime, timedelta
          import logging
          
          logger = logging.getLogger(__name__)
          
          @dataclass
          class LoadTestResult:
              """Load test result metrics"""
              test_name: str
              duration_seconds: float
              total_requests: int
              successful_requests: int
              failed_requests: int
              avg_response_time: float
              min_response_time: float
              max_response_time: float
              p95_response_time: float
              p99_response_time: float
              requests_per_second: float
              error_rate: float
              errors: List[str]
              
              def to_dict(self) -> Dict[str, Any]:
                  return {
                      'test_name': self.test_name,
                      'duration_seconds': self.duration_seconds,
                      'total_requests': self.total_requests,
                      'successful_requests': self.successful_requests,
                      'failed_requests': self.failed_requests,
                      'avg_response_time': self.avg_response_time,
                      'min_response_time': self.min_response_time,
                      'max_response_time': self.max_response_time,
                      'p95_response_time': self.p95_response_time,
                      'p99_response_time': self.p99_response_time,
                      'requests_per_second': self.requests_per_second,
                      'error_rate': self.error_rate,
                      'errors': self.errors[:10]  # Limit errors in output
                  }
          
          class LoadTester:
              """Load testing framework"""
              
              def __init__(
                  self,
                  base_url: str = "http://localhost:8001",
                  api_key: Optional[str] = None
              ):
                  self.base_url = base_url.rstrip('/')
                  self.api_key = api_key
                  self.session: Optional[aiohttp.ClientSession] = None
              
              async def __aenter__(self):
                  headers = {
                      "Content-Type": "application/json",
                      "User-Agent": "tahoe-load-tester/1.0"
                  }
                  
                  if self.api_key:
                      headers["X-API-Key"] = self.api_key
                  
                  connector = aiohttp.TCPConnector(limit=1000, limit_per_host=100)
                  self.session = aiohttp.ClientSession(
                      connector=connector,
                      headers=headers,
                      timeout=aiohttp.ClientTimeout(total=30)
                  )
                  return self
              
              async def __aexit__(self, exc_type, exc_val, exc_tb):
                  if self.session:
                      await self.session.close()
              
              async def make_request(
                  self,
                  method: str,
                  path: str,
                  json_data: Optional[Dict] = None
              ) -> tuple[bool, float, Optional[str]]:
                  """Make a single request and return success, response_time, error"""
                  start_time = time.time()
                  
                  try:
                      url = f"{self.base_url}/api/v1{path}"
                      
                      async with self.session.request(
                          method,
                          url,
                          json=json_data
                      ) as response:
                          await response.read()  # Ensure response is fully read
                          response_time = time.time() - start_time
                          
                          if response.status < 400:
                              return True, response_time, None
                          else:
                              error_text = await response.text()
                              return False, response_time, f"HTTP {response.status}: {error_text[:100]}"
                  
                  except Exception as e:
                      response_time = time.time() - start_time
                      return False, response_time, str(e)[:100]
              
              async def run_load_test(
                  self,
                  test_name: str,
                  request_func,
                  concurrent_users: int,
                  duration_seconds: int,
                  ramp_up_seconds: int = 0
              ) -> LoadTestResult:
                  """Run a load test scenario"""
                  
                  logger.info(f"Starting load test: {test_name}")
                  logger.info(f"Concurrent users: {concurrent_users}, Duration: {duration_seconds}s")
                  
                  results = []
                  errors = []
                  start_time = time.time()
                  end_time = start_time + duration_seconds
                  
                  # Create semaphore to limit concurrent requests
                  semaphore = asyncio.Semaphore(concurrent_users)
                  
                  async def worker():
                      """Worker coroutine for making requests"""
                      while time.time() < end_time:
                          async with semaphore:
                              success, response_time, error = await request_func()
                              results.append((success, response_time))
                              
                              if error:
                                  errors.append(error)
                              
                              # Small delay to prevent overwhelming the server
                              await asyncio.sleep(0.001)
                  
                  # Start workers with ramp-up
                  workers = []
                  for i in range(concurrent_users):
                      if ramp_up_seconds > 0:
                          await asyncio.sleep(ramp_up_seconds / concurrent_users)
                      workers.append(asyncio.create_task(worker()))
                  
                  # Wait for all workers to complete
                  await asyncio.gather(*workers, return_exceptions=True)
                  
                  # Calculate metrics
                  actual_duration = time.time() - start_time
                  total_requests = len(results)
                  successful_requests = sum(1 for success, _ in results if success)
                  failed_requests = total_requests - successful_requests
                  
                  if total_requests == 0:
                      logger.warning(f"No requests completed for test: {test_name}")
                      return LoadTestResult(
                          test_name=test_name,
                          duration_seconds=actual_duration,
                          total_requests=0,
                          successful_requests=0,
                          failed_requests=0,
                          avg_response_time=0.0,
                          min_response_time=0.0,
                          max_response_time=0.0,
                          p95_response_time=0.0,
                          p99_response_time=0.0,
                          requests_per_second=0.0,
                          error_rate=1.0,
                          errors=errors
                      )
                  
                  response_times = [rt for _, rt in results]
                  avg_response_time = statistics.mean(response_times)
                  min_response_time = min(response_times)
                  max_response_time = max(response_times)
                  
                  sorted_times = sorted(response_times)
                  p95_response_time = sorted_times[int(0.95 * len(sorted_times))]
                  p99_response_time = sorted_times[int(0.99 * len(sorted_times))]
                  
                  requests_per_second = total_requests / actual_duration
                  error_rate = failed_requests / total_requests
                  
                  result = LoadTestResult(
                      test_name=test_name,
                      duration_seconds=actual_duration,
                      total_requests=total_requests,
                      successful_requests=successful_requests,
                      failed_requests=failed_requests,
                      avg_response_time=avg_response_time,
                      min_response_time=min_response_time,
                      max_response_time=max_response_time,
                      p95_response_time=p95_response_time,
                      p99_response_time=p99_response_time,
                      requests_per_second=requests_per_second,
                      error_rate=error_rate,
                      errors=list(set(errors))  # Unique errors
                  )
                  
                  logger.info(f"Test {test_name} completed:")
                  logger.info(f"  Requests: {total_requests} ({successful_requests} success, {failed_requests} failed)")
                  logger.info(f"  RPS: {requests_per_second:.2f}")
                  logger.info(f"  Avg response time: {avg_response_time*1000:.2f}ms")
                  logger.info(f"  Error rate: {error_rate*100:.2f}%")
                  
                  return result
          
          class TahoeLoadTests:
              """Tahoe-specific load test scenarios"""
              
              def __init__(self, base_url: str = "http://localhost:8001", api_key: str = None):
                  self.base_url = base_url
                  self.api_key = api_key
              
              async def test_health_endpoint(self):
                  """Test health endpoint under load"""
                  async with LoadTester(self.base_url, self.api_key) as tester:
                      
                      async def health_request():
                          return await tester.make_request("GET", "/health")
                      
                      return await tester.run_load_test(
                          test_name="health_endpoint",
                          request_func=health_request,
                          concurrent_users=50,
                          duration_seconds=30
                      )
              
              async def test_agent_creation(self):
                  """Test agent creation under load"""
                  async with LoadTester(self.base_url, self.api_key) as tester:
                      
                      async def create_agent_request():
                          agent_spec = {
                              "name": f"Load Test Agent {time.time()}",
                              "type": "llm",
                              "specification": {
                                  "model": "gemini-1.5-pro",
                                  "system_instruction": "You are a test agent."
                              }
                          }
                          return await tester.make_request("POST", "/agents", agent_spec)
                      
                      return await tester.run_load_test(
                          test_name="agent_creation",
                          request_func=create_agent_request,
                          concurrent_users=10,
                          duration_seconds=60,
                          ramp_up_seconds=10
                      )
              
              async def test_session_management(self):
                  """Test session creation and management"""
                  async with LoadTester(self.base_url, self.api_key) as tester:
                      
                      async def create_session_request():
                          session_spec = {
                              "user_id": f"load_test_user_{time.time()}",
                              "app_name": "load_test_app"
                          }
                          return await tester.make_request("POST", "/sessions", session_spec)
                      
                      return await tester.run_load_test(
                          test_name="session_management",
                          request_func=create_session_request,
                          concurrent_users=25,
                          duration_seconds=45
                      )
              
              async def test_mixed_workload(self):
                  """Test mixed API workload"""
                  async with LoadTester(self.base_url, self.api_key) as tester:
                      
                      async def mixed_request():
                          import random
                          
                          # Weighted random selection of operations
                          operations = [
                              ("GET", "/health", None, 0.4),
                              ("GET", "/agents", None, 0.3),
                              ("GET", "/sessions", None, 0.2),
                              ("POST", "/sessions", {
                                  "user_id": f"user_{random.randint(1, 1000)}",
                                  "app_name": "load_test"
                              }, 0.1)
                          ]
                          
                          # Select operation based on weights
                          rand = random.random()
                          cumulative = 0
                          
                          for method, path, data, weight in operations:
                              cumulative += weight
                              if rand <= cumulative:
                                  return await tester.make_request(method, path, data)
                          
                          # Fallback to health check
                          return await tester.make_request("GET", "/health")
                      
                      return await tester.run_load_test(
                          test_name="mixed_workload",
                          request_func=mixed_request,
                          concurrent_users=30,
                          duration_seconds=120,
                          ramp_up_seconds=15
                      )
              
              async def run_all_tests(self) -> List[LoadTestResult]:
                  """Run all load tests"""
                  tests = [
                      self.test_health_endpoint(),
                      self.test_agent_creation(),
                      self.test_session_management(),
                      self.test_mixed_workload()
                  ]
                  
                  results = []
                  for test in tests:
                      try:
                          result = await test
                          results.append(result)
                          
                          # Brief pause between tests
                          await asyncio.sleep(5)
                      except Exception as e:
                          logger.error(f"Test failed: {e}")
                  
                  return results
          
          async def main():
              """Run load tests"""
              import os
              
              base_url = os.getenv("TAHOE_BASE_URL", "http://localhost:8001")
              api_key = os.getenv("TAHOE_API_KEY")
              
              if not api_key:
                  logger.warning("No API key provided. Some tests may fail.")
              
              load_tests = TahoeLoadTests(base_url, api_key)
              results = await load_tests.run_all_tests()
              
              # Generate summary report
              print("\n" + "="*80)
              print("LOAD TEST SUMMARY")
              print("="*80)
              
              for result in results:
                  print(f"\nTest: {result.test_name}")
                  print(f"  Duration: {result.duration_seconds:.1f}s")
                  print(f"  Total Requests: {result.total_requests:,}")
                  print(f"  Success Rate: {(result.successful_requests/result.total_requests)*100:.1f}%")
                  print(f"  Requests/sec: {result.requests_per_second:.2f}")
                  print(f"  Avg Response: {result.avg_response_time*1000:.1f}ms")
                  print(f"  95th percentile: {result.p95_response_time*1000:.1f}ms")
                  if result.errors:
                      print(f"  Sample errors: {result.errors[:3]}")
              
              # Save detailed results
              timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
              report_file = f"performance/reports/load_test_{timestamp}.json"
              
              os.makedirs("performance/reports", exist_ok=True)
              with open(report_file, 'w') as f:
                  json.dump([result.to_dict() for result in results], f, indent=2)
              
              print(f"\nDetailed results saved to: {report_file}")
          
          if __name__ == "__main__":
              logging.basicConfig(
                  level=logging.INFO,
                  format='%(asctime)s - %(levelname)s - %(message)s'
              )
              asyncio.run(main())
      
      - path: "performance/benchmarks/"
        purpose: "Performance benchmarks"
        content:
          - "agent_benchmarks.py"
          - "session_benchmarks.py"
          - "workflow_benchmarks.py"
          - "database_benchmarks.py"
      
      - path: "performance/benchmarks/agent_benchmarks.py"
        purpose: "Agent performance benchmarks"
        content_structure: |
          """Performance benchmarks for agent operations"""
          
          import asyncio
          import time
          import statistics
          from typing import List, Dict, Any
          import json
          from datetime import datetime
          import logging
          
          # Mock imports - would be actual agent components
          from services.agent_engine.src.core.agent_factory import AgentFactory
          from google.genai.adk import InMemoryRunner
          
          logger = logging.getLogger(__name__)
          
          class AgentBenchmarks:
              """Agent performance benchmarking suite"""
              
              def __init__(self):
                  self.factory = AgentFactory()
                  self.runner = InMemoryRunner()
                  self.results = []
              
              async def benchmark_agent_creation(self, iterations: int = 100) -> Dict[str, Any]:
                  """Benchmark agent creation performance"""
                  
                  agent_specs = [
                      {
                          "type": "llm",
                          "model": "gemini-1.5-pro",
                          "system_instruction": "You are a test agent."
                      },
                      {
                          "type": "sequential",
                          "steps": [
                              {
                                  "name": "step1",
                                  "agent": {
                                      "type": "llm",
                                      "model": "gemini-1.5-pro",
                                      "system_instruction": "Process input."
                                  }
                              }
                          ]
                      }
                  ]
                  
                  creation_times = []
                  
                  for i in range(iterations):
                      spec = agent_specs[i % len(agent_specs)]
                      
                      start_time = time.time()
                      try:
                          agent = self.factory.create_agent(spec)
                          creation_time = time.time() - start_time
                          creation_times.append(creation_time)
                      except Exception as e:
                          logger.error(f"Agent creation failed: {e}")
                  
                  if not creation_times:
                      return {"error": "No successful agent creations"}
                  
                  return {
                      "test": "agent_creation",
                      "iterations": len(creation_times),
                      "avg_time_ms": statistics.mean(creation_times) * 1000,
                      "min_time_ms": min(creation_times) * 1000,
                      "max_time_ms": max(creation_times) * 1000,
                      "p95_time_ms": statistics.quantiles(creation_times, n=20)[18] * 1000,
                      "agents_per_second": 1 / statistics.mean(creation_times)
                  }
              
              async def benchmark_agent_execution(self, iterations: int = 50) -> Dict[str, Any]:
                  """Benchmark agent execution performance"""
                  
                  # Create test agent
                  agent_spec = {
                      "type": "llm",
                      "model": "gemini-1.5-pro",
                      "system_instruction": "Return the input unchanged."
                  }
                  
                  agent = self.factory.create_agent(agent_spec)
                  execution_times = []
                  
                  test_inputs = [
                      {"text": "Hello world"},
                      {"text": "This is a test message"},
                      {"data": [1, 2, 3, 4, 5]},
                      {"question": "What is 2+2?"}
                  ]
                  
                  for i in range(iterations):
                      input_data = test_inputs[i % len(test_inputs)]
                      
                      start_time = time.time()
                      try:
                          result = await self.runner.run(agent, input_data)
                          execution_time = time.time() - start_time
                          execution_times.append(execution_time)
                      except Exception as e:
                          logger.error(f"Agent execution failed: {e}")
                  
                  if not execution_times:
                      return {"error": "No successful executions"}
                  
                  return {
                      "test": "agent_execution",
                      "iterations": len(execution_times),
                      "avg_time_ms": statistics.mean(execution_times) * 1000,
                      "min_time_ms": min(execution_times) * 1000,
                      "max_time_ms": max(execution_times) * 1000,
                      "p95_time_ms": statistics.quantiles(execution_times, n=20)[18] * 1000,
                      "executions_per_second": 1 / statistics.mean(execution_times)
                  }
              
              async def benchmark_concurrent_execution(self, concurrent_agents: int = 10) -> Dict[str, Any]:
                  """Benchmark concurrent agent executions"""
                  
                  # Create multiple agents
                  agents = []
                  for i in range(concurrent_agents):
                      spec = {
                          "type": "llm",
                          "model": "gemini-1.5-pro",
                          "system_instruction": f"You are agent {i}. Return the input with your ID."
                      }
                      agents.append(self.factory.create_agent(spec))
                  
                  # Execute all agents concurrently
                  start_time = time.time()
                  
                  tasks = []
                  for i, agent in enumerate(agents):
                      task = self.runner.run(agent, {"message": f"Test message {i}"})
                      tasks.append(task)
                  
                  results = await asyncio.gather(*tasks, return_exceptions=True)
                  
                  total_time = time.time() - start_time
                  successful_executions = sum(1 for r in results if not isinstance(r, Exception))
                  
                  return {
                      "test": "concurrent_execution",
                      "concurrent_agents": concurrent_agents,
                      "successful_executions": successful_executions,
                      "total_time_ms": total_time * 1000,
                      "avg_time_per_agent_ms": (total_time / concurrent_agents) * 1000,
                      "throughput_agents_per_second": concurrent_agents / total_time
                  }
              
              async def run_all_benchmarks(self) -> List[Dict[str, Any]]:
                  """Run all agent benchmarks"""
                  benchmarks = [
                      self.benchmark_agent_creation(),
                      self.benchmark_agent_execution(),
                      self.benchmark_concurrent_execution()
                  ]
                  
                  results = []
                  for benchmark in benchmarks:
                      try:
                          result = await benchmark
                          results.append(result)
                          logger.info(f"Completed benchmark: {result.get('test', 'unknown')}")
                      except Exception as e:
                          logger.error(f"Benchmark failed: {e}")
                  
                  return results
          
          async def main():
              """Run agent benchmarks"""
              benchmarks = AgentBenchmarks()
              results = await benchmarks.run_all_benchmarks()
              
              # Print results
              print("\n" + "="*60)
              print("AGENT PERFORMANCE BENCHMARKS")
              print("="*60)
              
              for result in results:
                  if "error" in result:
                      print(f"\nBenchmark failed: {result['error']}")
                      continue
                  
                  test_name = result.get("test", "unknown")
                  print(f"\nTest: {test_name}")
                  
                  if "iterations" in result:
                      print(f"  Iterations: {result['iterations']}")
                      print(f"  Average time: {result['avg_time_ms']:.2f}ms")
                      print(f"  95th percentile: {result['p95_time_ms']:.2f}ms")
                      
                      if "agents_per_second" in result:
                          print(f"  Throughput: {result['agents_per_second']:.2f} ops/sec")
                  
                  if "concurrent_agents" in result:
                      print(f"  Concurrent agents: {result['concurrent_agents']}")
                      print(f"  Successful: {result['successful_executions']}")
                      print(f"  Total time: {result['total_time_ms']:.2f}ms")
                      print(f"  Throughput: {result['throughput_agents_per_second']:.2f} agents/sec")
              
              # Save results
              timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
              results_file = f"performance/reports/agent_benchmarks_{timestamp}.json"
              
              import os
              os.makedirs("performance/reports", exist_ok=True)
              
              with open(results_file, 'w') as f:
                  json.dump(results, f, indent=2, default=str)
              
              print(f"\nResults saved to: {results_file}")
          
          if __name__ == "__main__":
              logging.basicConfig(
                  level=logging.INFO,
                  format='%(asctime)s - %(levelname)s - %(message)s'
              )
              asyncio.run(main())
      
      - path: "performance/scripts/"
        purpose: "Performance testing scripts"
        content:
          - "run_load_tests.sh"
          - "run_benchmarks.sh"
          - "generate_report.py"
          - "continuous_monitoring.py"
      
      - path: "performance/scripts/run_load_tests.sh"
        purpose: "Load testing execution script"
        content_structure: |
          #!/bin/bash
          set -e
          
          # Performance testing script for Tahoe
          
          SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
          PROJECT_DIR="$(dirname "$(dirname "$SCRIPT_DIR")")" 
          
          # Configuration
          TAHOE_URL=${TAHOE_URL:-"http://localhost:8001"}
          API_KEY=${TAHOE_API_KEY}
          RESULTS_DIR="$PROJECT_DIR/performance/reports"
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          
          echo "Starting Tahoe Performance Tests"
          echo "================================="
          echo "Target URL: $TAHOE_URL"
          echo "Results directory: $RESULTS_DIR"
          echo "Timestamp: $TIMESTAMP"
          
          # Create results directory
          mkdir -p "$RESULTS_DIR"
          
          # Export environment variables for tests
          export TAHOE_BASE_URL="$TAHOE_URL"
          export TAHOE_API_KEY="$API_KEY"
          
          # Function to check if Tahoe is healthy
          check_health() {
              echo "Checking Tahoe health..."
              
              local max_attempts=30
              local attempt=1
              
              while [ $attempt -le $max_attempts ]; do
                  if curl -f -s "$TAHOE_URL/health" > /dev/null 2>&1; then
                      echo "‚úÖ Tahoe is healthy"
                      return 0
                  fi
                  
                  echo "‚è≥ Waiting for Tahoe... (attempt $attempt/$max_attempts)"
                  sleep 5
                  attempt=$((attempt + 1))
              done
              
              echo "‚ùå Tahoe is not responding after $max_attempts attempts"
              return 1
          }
          
          # Function to run system monitoring in background
          start_monitoring() {
              echo "Starting system monitoring..."
              
              # Create monitoring script
              cat > "$RESULTS_DIR/monitor_$TIMESTAMP.sh" << 'EOF'
          #!/bin/bash
          RESULTS_DIR="$1"
          TIMESTAMP="$2"
          
          while true; do
              echo "$(date '+%Y-%m-%d %H:%M:%S'),$(top -l 1 -s 0 | grep 'CPU usage' | cut -d':' -f2 | cut -d'%' -f1 | tr -d ' '),$(top -l 1 -s 0 | grep 'PhysMem' | cut -d':' -f2 | cut -d'(' -f1 | tr -d ' ')" >> "$RESULTS_DIR/system_metrics_$TIMESTAMP.csv"
              sleep 5
          done
          EOF
              
              chmod +x "$RESULTS_DIR/monitor_$TIMESTAMP.sh"
              "$RESULTS_DIR/monitor_$TIMESTAMP.sh" "$RESULTS_DIR" "$TIMESTAMP" &
              MONITOR_PID=$!
              
              echo "System monitoring started (PID: $MONITOR_PID)"
              return $MONITOR_PID
          }
          
          # Function to stop monitoring
          stop_monitoring() {
              if [ ! -z "$MONITOR_PID" ]; then
                  kill $MONITOR_PID 2>/dev/null || true
                  echo "System monitoring stopped"
              fi
          }
          
          # Cleanup function
          cleanup() {
              echo "\nCleaning up..."
              stop_monitoring
              
              # Clean up temporary files
              rm -f "$RESULTS_DIR/monitor_$TIMESTAMP.sh"
          }
          
          # Set up cleanup on exit
          trap cleanup EXIT
          
          # Main execution
          main() {
              # Check health first
              if ! check_health; then
                  echo "‚ùå Cannot proceed without healthy Tahoe instance"
                  exit 1
              fi
              
              # Start monitoring
              start_monitoring
              
              echo "\nüìä Running Load Tests"
              echo "==================="
              
              # Run load tests
              cd "$PROJECT_DIR"
              
              echo "\n1. Basic load tests..."
              python -m performance.tests.load_tests
              
              echo "\n2. Stress tests..."
              if [ -f "performance/tests/stress_tests.py" ]; then
                  python -m performance.tests.stress_tests
              fi
              
              echo "\n3. Spike tests..."
              if [ -f "performance/tests/spike_tests.py" ]; then
                  python -m performance.tests.spike_tests
              fi
              
              echo "\nüìà Running Benchmarks"
              echo "==================="
              
              echo "\n1. Agent benchmarks..."
              python -m performance.benchmarks.agent_benchmarks
              
              echo "\n2. Session benchmarks..."
              if [ -f "performance/benchmarks/session_benchmarks.py" ]; then
                  python -m performance.benchmarks.session_benchmarks
              fi
              
              echo "\nüìã Generating Report"
              echo "=================="
              
              if [ -f "performance/scripts/generate_report.py" ]; then
                  python performance/scripts/generate_report.py "$RESULTS_DIR" "$TIMESTAMP"
              fi
              
              echo "\n‚úÖ Performance testing completed!"
              echo "üìÅ Results available in: $RESULTS_DIR"
              
              # List generated files
              echo "\nGenerated files:"
              ls -la "$RESULTS_DIR" | grep "$TIMESTAMP" || echo "No timestamped files found"
          }
          
          # Run main function
          main "$@"
      
      - path: "performance/config/"
        purpose: "Performance testing configuration"
        content:
          - "load_test_config.yaml"
          - "benchmark_config.yaml"
          - "thresholds.yaml"
      
      - path: "performance/config/thresholds.yaml"
        purpose: "Performance thresholds and SLAs"
        content_structure: |
          # Performance thresholds and SLA definitions
          
          api_endpoints:
            health:
              max_response_time_ms: 100
              min_success_rate: 99.9
              max_error_rate: 0.1
            
            agent_creation:
              max_response_time_ms: 2000
              min_success_rate: 95.0
              max_error_rate: 5.0
            
            agent_execution:
              max_response_time_ms: 10000
              min_success_rate: 90.0
              max_error_rate: 10.0
            
            session_creation:
              max_response_time_ms: 1000
              min_success_rate: 95.0
              max_error_rate: 5.0
          
          load_testing:
            concurrent_users:
              light_load: 10
              normal_load: 50
              heavy_load: 100
              stress_load: 200
            
            duration_seconds:
              smoke_test: 30
              load_test: 300
              stress_test: 600
              endurance_test: 3600
          
          system_resources:
            cpu_usage:
              warning_threshold: 70
              critical_threshold: 85
            
            memory_usage:
              warning_threshold: 70
              critical_threshold: 85
            
            response_times:
              p50_threshold_ms: 500
              p95_threshold_ms: 2000
              p99_threshold_ms: 5000
          
          benchmarks:
            agent_creation:
              target_ops_per_second: 100
              max_avg_time_ms: 10
            
            agent_execution:
              target_ops_per_second: 10
              max_avg_time_ms: 1000
            
            session_operations:
              target_ops_per_second: 200
              max_avg_time_ms: 5
          
          alerts:
            error_rate_threshold: 5.0  # percent
            response_time_threshold: 2000  # ms
            availability_threshold: 99.0  # percent
      
      - path: "performance/tests/test_performance.py"
        purpose: "Performance test validation"
        test_categories:
          - "Load test execution"
          - "Benchmark accuracy"
          - "Threshold validation"
          - "Report generation"
          - "Monitoring integration"
    
    uses_from_previous:
      - source: "r7-t01"
        component: "Docker deployment"
        usage: "Performance testing target"
      - source: "r7-t02"
        component: "Monitoring system"
        usage: "Performance metrics collection"
      - source: "r6-t04"
        component: "Authentication system"
        usage: "Authenticated performance testing"
    
  implementation_steps:
    - step: "Create load testing framework"
      implementation_notes: |
        - Async HTTP client
        - Concurrent request handling
        - Metrics collection
        
    - step: "Build benchmark suites"
      implementation_notes: |
        - Agent performance tests
        - Session benchmarks
        - Database performance
        
    - step: "Implement test scenarios"
      implementation_notes: |
        - Load testing patterns
        - Stress testing
        - Spike testing
        
    - step: "Create monitoring integration"
      implementation_notes: |
        - System metrics collection
        - Performance dashboards
        - Alert thresholds
        
    - step: "Build reporting system"
      implementation_notes: |
        - Performance reports
        - Trend analysis
        - Optimization recommendations
        
  validation:
    commands:
      - description: "Run performance tests"
        command: "cd performance && python tests/test_performance.py"
        expected: "All tests pass"
        
      - description: "Execute load tests"
        command: "./performance/scripts/run_load_tests.sh"
        expected: "Load tests complete successfully"
        
      - description: "Run benchmarks"
        command: "python performance/benchmarks/agent_benchmarks.py"
        expected: "Benchmarks execute and report metrics"
        
      - description: "Validate thresholds"
        command: "python -c \"import yaml; print(yaml.safe_load(open('performance/config/thresholds.yaml')))\""
        expected: "Configuration loads without errors"
        
    success_criteria:
      - "Load testing framework operational"
      - "Benchmarks provide accurate metrics"
      - "Performance thresholds defined"
      - "Monitoring integration complete"
      - "Reports generate successfully"
      
  dependencies:
    required_before:
      - task: "r7-t01"
        reason: "Need deployment target for testing"
      - task: "r7-t02"
        reason: "Need monitoring for metrics collection"
      - task: "r6-t04"
        reason: "Need authentication for API testing"