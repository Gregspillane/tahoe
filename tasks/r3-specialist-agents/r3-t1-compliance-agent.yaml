task:
  id: "r3-t1-compliance-agent"
  name: "Build Compliance Specialist Agent"
  session_size: "Complete FDCPA, TCPA, and Reg F compliance detection agent"
  
  context:
    why: "Core compliance analysis capability detecting regulatory violations with evidence"
    integrates_with: ["agent-factory", "orchestration-engine", "result-aggregation"]
    references:
      - "masterplan compliance specialist (lines 1970-2000+)"
      - "roadmap phase 3 compliance requirements"
    
  build_spec:
    creates:
      - "services/agent-engine/src/agents/specialists/__init__.py"
      - "services/agent-engine/src/agents/specialists/compliance.py"
      - "services/agent-engine/src/data/regulations/fdcpa_rules.json"
      - "services/agent-engine/src/data/regulations/tcpa_rules.json"
      - "services/agent-engine/src/data/regulations/reg_f_rules.json"
      - "services/agent-engine/tests/test_compliance_agent.py"
      - "services/agent-engine/tests/fixtures/compliance_violations.json"
    modifies:
      - "scripts/seed.py (add compliance agent template)"
      - "src/agents/factory.py (register specialist)"
    uses:
      - "Google ADK for LLM-powered analysis"
      - "Gemini models for compliance detection"
      - "Regulation rule sets for context"
      - "LLM-based evidence extraction"
      - "AI-powered severity classification"
    
  implementation_guide:
    start_with: "Create ComplianceSpecialistAgent class extending BaseSpecialistAgent"
    core_logic:
      - "Load regulation rule sets as LLM context"
      - "Use Gemini to analyze transcript for violations"
      - "LLM-powered evidence extraction with citations"
      - "AI-driven severity score calculation"
      - "Generate compliance score via LLM reasoning"
      - "LLM-generated remediation recommendations"
      - "Create detailed violation records with AI analysis"
    connects_to:
      - "Base agent framework"
      - "Orchestrator for execution"
      - "Aggregator for results"
    keeps_simple:
      - "LLM-powered analysis with structured prompts"
      - "Regulation rules as context, not hard-coded logic"
      - "Single-pass Gemini analysis for efficiency"
      - "Structured output parsing from LLM responses"
      - "Focus on FDCPA, TCPA, and Reg F regulations"
    
  local_validation:
    run_commands:
      - "python -m pytest tests/test_compliance_agent.py -v"
      - "python scripts/test_compliance_detection.py"
    verify_endpoints:
      - "Agent detects known violations"
      - "Evidence includes context"
      - "Severity calculated correctly"
    check_functionality:
      - "FDCPA violations detected"
      - "TCPA violations identified"
      - "Reg F requirements checked"
      - "False positives minimized"
      - "Recommendations actionable"
    
  session_notes:
    context_critical:
      - "Regulations have specific requirements"
      - "Evidence needs surrounding context"
      - "Severity affects overall score"
      - "False positives harm credibility"
    remember_for_later:
      - "Rule sets will expand over time"
      - "ML enhancement possible later"
      - "Integration with legal databases"
      - "Jurisdiction considerations"

  implementation_checklist:
    - "Create ComplianceSpecialistAgent class"
    - "Load regulation rules from JSON"
    - "Implement violation detection"
    - "Build evidence extraction"
    - "Add severity calculation"
    - "Create recommendation engine"
    - "Write regulation data files"
    - "Add comprehensive tests"
    - "Create violation fixtures"

  regulation_rules:
    fdcpa_rules:
      - category: "harassment"
        patterns:
          - "repeated calls"
          - "threatening language"
          - "profanity"
        severity: "high"
        evidence_window: 50  # words around violation
      
      - category: "misrepresentation"
        patterns:
          - "false identity"
          - "incorrect debt amount"
          - "unauthorized fees"
        severity: "high"
        evidence_window: 100
      
      - category: "disclosure"
        patterns:
          - "third party disclosure"
          - "workplace contact"
          - "public shaming"
        severity: "critical"
        evidence_window: 75
    
    tcpa_rules:
      - category: "consent"
        patterns:
          - "no prior consent"
          - "autodialer usage"
          - "prerecorded message"
        severity: "high"
        evidence_window: 50
      
      - category: "timing"
        patterns:
          - "before 8am"
          - "after 9pm"
          - "repeated calls"
        severity: "medium"
        evidence_window: 30
    
    reg_f_rules:
      - category: "communication_frequency"
        patterns:
          - "more than 7 attempts per week"
          - "within 7 days of conversation"
        severity: "medium"
        evidence_window: 100
      
      - category: "electronic_communication"
        patterns:
          - "email without consent"
          - "text without opt-in"
        severity: "medium"
        evidence_window: 50

  violation_detection:
    pattern_matching: |
      violations = []
      for rule in regulations[regulation_type]:
          for pattern in rule["patterns"]:
              if pattern.lower() in transcript.lower():
                  violation = {
                      "type": f"{regulation_type}_{rule['category']}",
                      "pattern": pattern,
                      "severity": rule["severity"],
                      "evidence": extract_evidence(
                          transcript, 
                          pattern, 
                          rule["evidence_window"]
                      ),
                      "location": find_location(transcript, pattern),
                      "regulation": regulation_type.upper(),
                      "category": rule["category"]
                  }
                  violations.append(violation)
    
    evidence_extraction: |
      def extract_evidence(text, pattern, window):
          # Find pattern location
          idx = text.lower().find(pattern.lower())
          if idx == -1:
              return ""
          
          # Extract surrounding context
          start = max(0, idx - window)
          end = min(len(text), idx + len(pattern) + window)
          
          # Return with highlighting
          evidence = text[start:end]
          return {
              "text": evidence,
              "pattern_start": idx - start,
              "pattern_end": idx - start + len(pattern)
          }

  scoring_logic:
    compliance_score: |
      # Start at 100, deduct for violations
      score = 100.0
      
      severity_deductions = {
          "critical": 30,
          "high": 20,
          "medium": 10,
          "low": 5
      }
      
      for violation in violations:
          deduction = severity_deductions.get(
              violation["severity"], 5
          )
          score -= deduction
      
      # Ensure score stays in bounds
      return max(0, min(100, score))

  recommendations:
    generation: |
      recommendations = []
      
      for violation in violations:
          rec = {
              "id": f"rec_{violation['type']}",
              "violation_ref": violation["type"],
              "action": get_remediation_action(violation),
              "priority": map_severity_to_priority(violation["severity"]),
              "impact": "high" if violation["severity"] in ["critical", "high"] else "medium",
              "description": generate_recommendation_text(violation)
          }
          recommendations.append(rec)
      
      return recommendations