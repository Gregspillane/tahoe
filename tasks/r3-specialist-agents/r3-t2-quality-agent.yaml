task:
  id: "r3-t2-quality-agent"
  name: "Build Quality Assessment Agent"
  session_size: "Complete interaction quality scoring across 7 dimensions"
  
  context:
    why: "Evaluates communication effectiveness and customer experience quality"
    integrates_with: ["agent-factory", "orchestration-engine", "result-aggregation"]
    references:
      - "masterplan quality assessment requirements"
      - "roadmap phase 3 quality scoring"
    
  build_spec:
    creates:
      - "services/agent-engine/src/agents/specialists/quality.py"
      - "services/agent-engine/src/data/quality/assessment_criteria.json"
      - "services/agent-engine/src/data/quality/scoring_rubric.json"
      - "services/agent-engine/tests/test_quality_agent.py"
      - "services/agent-engine/tests/fixtures/quality_scenarios.json"
    modifies:
      - "scripts/seed.py (add quality agent template)"
      - "src/agents/factory.py (register specialist)"
    uses:
      - "Google ADK for LLM-powered quality assessment"
      - "Gemini models for communication analysis"
      - "Multi-dimensional scoring rubric as LLM context"
      - "AI-driven sentiment and empathy detection"
      - "LLM-based professional standards evaluation"
    
  implementation_guide:
    start_with: "Create QualityAssessmentAgent class extending BaseSpecialistAgent"
    core_logic:
      - "Use Gemini to score across 7 quality dimensions"
      - "LLM-powered communication pattern analysis"
      - "AI evaluation of professionalism indicators"
      - "Gemini assessment of empathy and rapport"
      - "LLM-driven resolution effectiveness analysis"
      - "AI measurement of clarity and accuracy"
      - "Generate LLM-powered improvement suggestions"
    connects_to:
      - "Base agent framework"
      - "Orchestrator for execution"
      - "Aggregator for results"
    keeps_simple:
      - "Single Gemini call with structured quality analysis prompt"
      - "Scoring rubric provided as LLM context"
      - "Structured JSON output from LLM"
      - "Focus on 7 core quality dimensions"
      - "LLM-generated scores with reasoning"
    
  local_validation:
    run_commands:
      - "python -m pytest tests/test_quality_agent.py -v"
      - "python scripts/test_quality_scoring.py"
    verify_endpoints:
      - "Agent scores all dimensions"
      - "Overall quality score calculated"
      - "Recommendations generated"
    check_functionality:
      - "Each dimension scored 0-100"
      - "Weighted average correct"
      - "Low scores trigger recommendations"
      - "Patterns detected accurately"
      - "Professional standards checked"
    
  session_notes:
    context_critical:
      - "7 dimensions cover full interaction"
      - "Weights determine importance"
      - "Context affects scoring"
      - "Industry standards apply"
    remember_for_later:
      - "Scoring rubric will evolve"
      - "ML enhancement for sentiment"
      - "Industry-specific criteria"
      - "Calibration over time"

  implementation_checklist:
    - "Create QualityAssessmentAgent class"
    - "Define 7 quality dimensions"
    - "Build scoring rubric"
    - "Implement pattern analysis"
    - "Add dimension scoring"
    - "Calculate weighted average"
    - "Generate recommendations"
    - "Write quality criteria files"
    - "Create comprehensive tests"

  quality_dimensions:
    1_professionalism:
      weight: 0.15
      indicators:
        positive:
          - "proper greeting"
          - "polite language"
          - "respectful tone"
          - "appropriate closing"
        negative:
          - "profanity"
          - "sarcasm"
          - "dismissive language"
          - "interruptions"
      scoring: "Start at 70, +5 for positive, -10 for negative"
    
    2_empathy:
      weight: 0.15
      indicators:
        positive:
          - "acknowledging concerns"
          - "expressing understanding"
          - "showing patience"
          - "active listening"
        negative:
          - "dismissing feelings"
          - "rushed responses"
          - "lack of acknowledgment"
      scoring: "Count positive indicators, scale to 100"
    
    3_clarity:
      weight: 0.15
      indicators:
        positive:
          - "clear explanations"
          - "avoiding jargon"
          - "confirming understanding"
          - "summarizing key points"
        negative:
          - "confusing language"
          - "technical terms without explanation"
          - "contradictions"
      scoring: "Ratio of clear vs unclear statements"
    
    4_accuracy:
      weight: 0.15
      indicators:
        positive:
          - "correct information"
          - "accurate amounts"
          - "valid dates"
          - "proper procedures"
        negative:
          - "incorrect facts"
          - "wrong amounts"
          - "misinformation"
      scoring: "Deduct heavily for inaccuracies"
    
    5_resolution:
      weight: 0.15
      indicators:
        positive:
          - "problem identified"
          - "solution offered"
          - "next steps clear"
          - "timeline provided"
        negative:
          - "unresolved issues"
          - "no clear outcome"
          - "passing responsibility"
      scoring: "Check resolution completeness"
    
    6_compliance_adherence:
      weight: 0.15
      indicators:
        positive:
          - "required disclosures"
          - "proper identification"
          - "consent obtained"
          - "rights explained"
        negative:
          - "missing disclosures"
          - "skipped requirements"
          - "regulatory violations"
      scoring: "Binary checks for requirements"
    
    7_efficiency:
      weight: 0.10
      indicators:
        positive:
          - "concise communication"
          - "staying on topic"
          - "timely responses"
          - "avoiding repetition"
        negative:
          - "excessive length"
          - "off-topic discussions"
          - "unnecessary delays"
      scoring: "Based on interaction length and focus"

  scoring_implementation:
    dimension_scoring: |
      def score_dimension(transcript, dimension_config):
          score = dimension_config.get("base_score", 70)
          
          # Check positive indicators
          for indicator in dimension_config["indicators"]["positive"]:
              if indicator.lower() in transcript.lower():
                  score += dimension_config.get("positive_weight", 5)
          
          # Check negative indicators
          for indicator in dimension_config["indicators"]["negative"]:
              if indicator.lower() in transcript.lower():
                  score -= dimension_config.get("negative_weight", 10)
          
          # Ensure bounds
          return max(0, min(100, score))
    
    overall_calculation: |
      def calculate_overall_quality(dimension_scores):
          weighted_sum = 0
          total_weight = 0
          
          for dimension, config in dimensions.items():
              score = dimension_scores[dimension]
              weight = config["weight"]
              weighted_sum += score * weight
              total_weight += weight
          
          return weighted_sum / total_weight if total_weight > 0 else 0

  recommendations_logic:
    low_score_triggers: |
      def generate_quality_recommendations(dimension_scores):
          recommendations = []
          
          for dimension, score in dimension_scores.items():
              if score < 70:  # Threshold for improvement
                  rec = {
                      "id": f"quality_{dimension}",
                      "dimension": dimension,
                      "current_score": score,
                      "target_score": 80,
                      "action": get_improvement_action(dimension),
                      "priority": "high" if score < 50 else "medium",
                      "impact": calculate_impact(dimension),
                      "training_module": suggest_training(dimension)
                  }
                  recommendations.append(rec)
          
          return sorted(recommendations, key=lambda x: x["current_score"])

  quality_patterns:
    communication_flow: |
      # Analyze conversation structure
      has_greeting = check_pattern(transcript, greeting_patterns)
      has_closing = check_pattern(transcript, closing_patterns)
      has_summary = check_pattern(transcript, summary_patterns)
      
      structure_score = sum([
          has_greeting * 30,
          has_closing * 30,
          has_summary * 40
      ])
    
    empathy_detection: |
      empathy_phrases = [
          "I understand",
          "I can see why",
          "That must be",
          "I appreciate",
          "Thank you for"
      ]
      
      empathy_count = sum(
          1 for phrase in empathy_phrases 
          if phrase.lower() in transcript.lower()
      )